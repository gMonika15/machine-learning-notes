{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Regression Techniques\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#Classfication Techniques\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Ensemble Techniques\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import balanced_accuracy_score #average aaccuracy of each class - imbalanced\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Over Sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#UnderSampling\n",
    "\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import add_constant\n",
    "\n",
    "#Data Preprocessing\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Data Visualisation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "print('Test',lr.score(x_test,y_test))\n",
    "print('Train',lr.score(x_train,y_train))\n",
    "\n",
    "## Ridge \n",
    "\n",
    "ridge = Ridge(alpha=1)\n",
    "ridge.fit(x_train,y_train)\n",
    "print('Test',ridge.score(x_test, y_test))\n",
    "print('Train',ridge.score(x_train, y_train))\n",
    "\n",
    "## Lasso\n",
    "\n",
    "lasso = Lasso(alpha=1)\n",
    "lasso.fit(x_train,y_train)\n",
    "print('Test',lasso.score(x_test, y_test))\n",
    "print('Train',lasso.score(x_train, y_train))\n",
    "\n",
    "print('Linear Regression Co-efficients:\\n',lr.coef_,'\\n\\n')\n",
    "print('Ridge Regression Co-efficients:\\n',ridge.coef_,'\\n\\n')\n",
    "print('Lasso Regression Co-efficients:\\n',lasso.coef_,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial Feautures\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree = 2, interaction_only=True)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Non_diabetic','Diabetic']\n",
    "plt.title('Confusion Matrix - Test Data')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    " \n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "#Accuracy:\n",
    "print((TP + TN) / float(TP + TN + FP + FN))\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "#Error\n",
    "print((FP + FN) / float(TP + TN + FP + FN))\n",
    "print(1 - metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "#Recall\n",
    "print(TP / float(TP + FN))\n",
    "print(metrics.recall_score(y_test, y_pred_class))\n",
    "\n",
    "#Specificity\n",
    "print((TN / float(TN + FP)))\n",
    "\n",
    "#FPR:\n",
    "print(FP / float(TN + FP))\n",
    "\n",
    "#Precision:\n",
    "print(TP / float(TP + FP))\n",
    "print(metrics.precision_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict diabetes if the predicted probability is greater than 0.3\n",
    "#Changing Threshold\n",
    "\n",
    "from sklearn.preprocessing import binarize\n",
    "y_pred_class = binarize([y_pred_prob], 0.3)[0]  # deciding the class of the 1st 10 records based on new threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: ROC_AUC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for diabetes classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric\n",
    "\n",
    "# calculate cross-validated AUC\n",
    "\n",
    "cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling - Not ensemble\n",
    "\n",
    "#After applying dimensionality reduction using p-value,...\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "\n",
    "print('Logistic Regression:')\n",
    "lr = LogisticRegression(random_state=1)\n",
    "lr.fit(x_train,y_train)\n",
    "print('Training:\\t',lr.score(x_train,y_train))\n",
    "print('Validation:\\t',lr.score(x_val,y_val))\n",
    "print('Testing:\\t',lr.score(x_test,y_test))\n",
    "\n",
    "#Decision Tree\n",
    "\n",
    "print('\\nDecision Tree:')\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(x_train,y_train)\n",
    "print('Training:\\t',dt.score(x_train,y_train))\n",
    "print('Validation:\\t',dt.score(x_val,y_val))\n",
    "print('Testing:\\t',dt.score(x_test,y_test))\n",
    "\n",
    "#Naive Bayes\n",
    "\n",
    "print('\\nNaive Bayes:')\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train,y_train)\n",
    "print('Training:\\t',nb.score(x_train,y_train))\n",
    "print('Validation:\\t',nb.score(x_val,y_val))\n",
    "print('Testing:\\t',nb.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure bootstrap\n",
    "values=np.array(values)\n",
    "n_iterations = 50           # Number of bootstrap samples to create\n",
    "n_size = int(len(values) * 0.50)    # picking only 50 % of the given data in every bootstrap sample\n",
    "\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "\t# prepare train and test sets\n",
    "\ttrain = resample(values, n_samples=n_size)  # Sampling with replacement\n",
    "\ttest = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "    # fit model\n",
    "\tmodel = lr\n",
    "\tmodel.fit(train[:,:-1],train[:,-1])\n",
    "    # evaluate model    \n",
    "\tscore = model.score(test[:,:-1], test[:,-1])   # caution, overall accuracy score can mislead when classes are imbalanced\n",
    "\tprint(score)\n",
    "\tstats.append(score)\n",
    "\n",
    "# plot scores\n",
    "plt.xticks()\n",
    "plt.hist(stats)\n",
    "plt.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95                             # for 95% confidence \n",
    "p = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\n",
    "lower = max(0.0, np.percentile(stats, p))  \n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelling - Ensemble\n",
    "\n",
    "#Model that we selected\n",
    "#Decision Tree\n",
    "\n",
    "print('\\nDecision Tree:')\n",
    "dt = DecisionTreeClassifier(random_state=1,max_depth=5)\n",
    "dt.fit(x_train,y_train)\n",
    "print('Training:\\t',dt.score(x_train,y_train))\n",
    "print('Validation:\\t',dt.score(x_val,y_val))\n",
    "print('Testing:\\t',dt.score(x_test,y_test))\n",
    "\n",
    "#Random Forest\n",
    "\n",
    "print('\\nRandom Forest:')\n",
    "rf = RandomForestClassifier(random_state=1,n_estimators=50)\n",
    "rf.fit(x_train,y_train)\n",
    "print('Training:\\t',rf.score(x_train,y_train))\n",
    "print('Validation:\\t',rf.score(x_val,y_val))\n",
    "print('Testing:\\t',rf.score(x_test,y_test))\n",
    "\n",
    "#Bagging Classifier\n",
    "\n",
    "print('\\nBagging Classifier:')\n",
    "bc = BaggingClassifier(random_state=1,n_estimators=50)\n",
    "bc.fit(x_train,y_train)\n",
    "print('Training:\\t',bc.score(x_train,y_train))\n",
    "print('Validation:\\t',bc.score(x_val,y_val))\n",
    "print('Testing:\\t',bc.score(x_test,y_test))\n",
    "\n",
    "#AdaBoosting Classifier\n",
    "\n",
    "print('\\nAdaBoost Classifier:')\n",
    "ab = AdaBoostClassifier(random_state=1,n_estimators=50)\n",
    "ab.fit(x_train,y_train)\n",
    "print('Training:\\t',ab.score(x_train,y_train))\n",
    "print('Validation:\\t',ab.score(x_val,y_val))\n",
    "print('Testing:\\t',ab.score(x_test,y_test))\n",
    "\n",
    "#GradientBoosting Classifier\n",
    "\n",
    "print('\\nGradientBoosting Classifier:')\n",
    "gb = GradientBoostingClassifier(random_state=1,n_estimators=50)\n",
    "gb.fit(x_train,y_train)\n",
    "print('Training:\\t',gb.score(x_train,y_train))\n",
    "print('Validation:\\t',gb.score(x_val,y_val))\n",
    "print('Testing:\\t',gb.score(x_test,y_test))\n",
    "\n",
    "#XGBoost\n",
    "\n",
    "print('\\nXGBoost:')\n",
    "xg = xgb.XGBRegressor()\n",
    "xg.fit(x_train,y_train)\n",
    "print('Training:\\t',xg.score(x_train,y_train))\n",
    "print('Validation:\\t',xg.score(x_val,y_val))\n",
    "print('Testing:\\t',xg.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Comparison\n",
    "\n",
    "models=[]\n",
    "models.append(('Logistic Regression',lr))\n",
    "models.append(('DecisionTree',dt))\n",
    "models.append(('RandomForest',rf))\n",
    "models.append(('AdaBoost',ab))\n",
    "models.append(('Gradient',gb))\n",
    "\n",
    "results=[]\n",
    "names=[]\n",
    "\n",
    "scoring='accuracy'\n",
    "for name,model in models:\n",
    "    kfold=model_selection.KFold(n_splits=5,random_state=1)\n",
    "    cv_results=model_selection.cross_val_score(model,X,Y,cv=kfold,scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg=\"%s: %f (%f)\"%(name,cv_results.mean(),cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax=fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column Transformer\n",
    "\n",
    "ct = ColumnTransformer([(\"scaling\", StandardScaler(), ['age' , 'hours-per-week']), (\"OneHotCoding\", OneHotEncoder(sparse=False),\n",
    "                                                                                  ['workclass', 'education', 'gender', 'occupation'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bin Discretizer\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "kb = KBinsDiscretizer(n_bins=10, strategy='uniform')\n",
    "\n",
    "kb.fit(X)\n",
    "\n",
    "print(\"bin edges \\n\", kb.bin_edges_)\n",
    "\n",
    "# With the bins defined, we can transform each data point X into a bin using the transform function\n",
    "\n",
    "X_binned = kb.transform(X)\n",
    "\n",
    "print(X_binned.toarray()[0,:])\n",
    "print(pd.DataFrame(X).head(1))   #First data point -0.752759 is stored in bin 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent sparse matrix and create dense matrix, let us re-do the binning with onehot coding. A given input belongs to\n",
    "# one bin and not others. Hence, we can onehot code them\n",
    "\n",
    "kb = KBinsDiscretizer( n_bins = 10, strategy ='uniform', encode ='onehot-dense') \n",
    "kb.fit(X) \n",
    "X_binned = kb.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random SearchCV\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "samples = 10  # number of random samples \n",
    "randomCV = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=samples) #default cv = 3\n",
    "\n",
    "randomCV.fit(X, y)\n",
    "\n",
    "print(randomCV.best_params_)\n",
    "#print(randomCV.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "grid_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgb takes data in matrix form both for training and testing...\n",
    "\n",
    "DM_train = xgb.DMatrix(data = X_train, \n",
    "                       label = y_train)  \n",
    "DM_test =  xgb.DMatrix(data = X_test,\n",
    "                       label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the hyper parameters ... Ref https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "\n",
    "gbm_param_grid = {\n",
    "     'colsample_bytree': np.linspace(0.5, 0.9, 5),  # generate 5 numbers between .5 and .9 \n",
    "     'n_estimators':[10, 200],\n",
    "     'max_depth': [10, 15, 20, 25]\n",
    "}\n",
    "\n",
    "gbm = xgb.XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator = gbm, param_grid = gbm_param_grid, scoring = 'neg_mean_squared_error', cv = 5, verbose = 1)\n",
    "grid_mse.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \",grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "\n",
    "pred = grid_mse.predict(X_test)\n",
    "print(\"Root mean square error for test dataset: {}\".format(np.round(np.sqrt(mean_squared_error(y_test, pred)), 2)))\n",
    "\n",
    "test = pd.DataFrame({\"prediction\": pred, \"observed\": y_test.flatten()})\n",
    "lowess = sm.nonparametric.lowess\n",
    "z = lowess(pred.flatten(), y_test.flatten())\n",
    "test.plot(figsize = [14,8],\n",
    "          x =\"prediction\", y = \"observed\", kind = \"scatter\", color = 'darkred')\n",
    "plt.title(\"Extreme Gradient Boosting: Prediction Vs Test Data\", fontsize = 18, color = \"darkgreen\")\n",
    "plt.xlabel(\"Predicted Power Output\", fontsize = 18) \n",
    "plt.ylabel(\"Observed Power Output\", fontsize = 18)\n",
    "plt.plot(z[:,0], z[:,1], color = \"blue\", lw= 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each model\n",
    "\n",
    "# configure bootstrap\n",
    "n_iterations = 100           # Number of bootstrap samples to create\n",
    "n_size = int(len(data) * 0.50)    # picking only 50 % of the given data in every bootstrap sample\n",
    "\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "\t# prepare train and test sets\n",
    "\ttrain = resample(values, n_samples=n_size)  # Sampling with replacement \n",
    "\ttest = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n",
    "    # fit model\n",
    "\tmodel = DecisionTreeClassifier()\n",
    "\tmodel.fit(train[:,:-1], train[:,-1])\n",
    "    # evaluate model\n",
    "\tpredictions = model.predict(test[:,:-1])\n",
    "\tscore = accuracy_score(test[:,-1], predictions)    # caution, overall accuracy score can mislead when classes are imbalanced\n",
    "\tprint(score)\n",
    "\tstats.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores\n",
    "pyplot.xticks()\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95                             # for 95% confidence \n",
    "p = ((1.0-alpha)/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\n",
    "lower = max(0.0, np.percentile(stats, p))  \n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
